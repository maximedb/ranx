{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\udd25 News ranx will be featured in ECIR 2022 , the 44th European Conference on Information Retrieval! Check out the new examples on Google Colab! Added a changelog to document few breaking changes introduced in v.0.1.11 . \u26a1\ufe0f Introduction ranx is a library of fast ranking evaluation metrics implemented in Python , leveraging Numba for high-speed vector operations and automatic parallelization. It offers a user-friendly interface to evaluate and compare Information Retrieval and Recommender Systems . Moreover, ranx allows you to perform statistical tests and export LaTeX tables for your scientific publications. For a quick overview, follow the Usage section. For a in-depth overview, follow the Examples section. \u2728 Available Metrics Hits Hit Rate Precision Recall F1 r-Precision Mean Reciprocal Rank (MRR) Mean Average Precision (MAP) Normalized Discounted Cumulative Gain (NDCG) The metrics have been tested against TREC Eval for correctness. \ud83d\udd0c Installation pip install ranx \ud83d\udca1 Usage Create Qrels and Run from ranx import Qrels , Run , evaluate qrels_dict = { \"q_1\" : { \"d_12\" : 5 , \"d_25\" : 3 }, \"q_2\" : { \"d_11\" : 6 , \"d_22\" : 1 } } run_dict = { \"q_1\" : { \"d_12\" : 0.9 , \"d_23\" : 0.8 , \"d_25\" : 0.7 , \"d_36\" : 0.6 , \"d_32\" : 0.5 , \"d_35\" : 0.4 }, \"q_2\" : { \"d_12\" : 0.9 , \"d_11\" : 0.8 , \"d_25\" : 0.7 , \"d_36\" : 0.6 , \"d_22\" : 0.5 , \"d_35\" : 0.4 } } qrels = Qrels ( qrels_dict ) run = Run ( run_dict ) Evaluate # Compute score for a single metric evaluate ( qrels , run , \"ndcg@5\" ) >>> 0.7861 # Compute scores for multiple metrics at once evaluate ( qrels , run , [ \"map@5\" , \"mrr\" ]) >>> { \"map@5\" : 0.6416 , \"mrr\" : 0.75 } Compare # Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: >>> # Model MAP@100 MRR@100 NDCG@10 --- ------- -------- -------- --------- a model_1 0.320\u1d47 0.320\u1d47 0.368\u1d47\u1d9c b model_2 0.233 0.234 0.239 c model_3 0.308\u1d47 0.309\u1d47 0.330\u1d47 d model_4 0.366\u1d43\u1d47\u1d9c 0.367\u1d43\u1d47\u1d9c 0.408\u1d43\u1d47\u1d9c e model_5 0.405\u1d43\u1d47\u1d9c\u1d48 0.406\u1d43\u1d47\u1d9c\u1d48 0.451\u1d43\u1d47\u1d9c\u1d48 \ud83d\udcd6 Examples Name Link Overview Qrels and Run Evaluation Comparison and Report \ud83d\udcda Documentation Browse the documentation for more details and examples. \ud83c\udf93 Citation If you use ranx to evaluate results for your scientific publication, please consider citing it: @misc{ranx2021, title = {ranx: A Blazing-Fast Python Library for Ranking Evaluation and Comparison}, author = {Bassani, Elias}, year = {2021}, publisher = {GitHub}, howpublished = {\\url{https://github.com/AmenRa/ranx}}, } \ud83c\udf81 Feature Requests Would you like to see other features implemented? Please, open a feature request . \ud83e\udd18 Want to contribute? Would you like to contribute? Please, drop me an e-mail . \ud83d\udcc4 License ranx is an open-sourced software licensed under the MIT license .","title":"Home"},{"location":"#news","text":"ranx will be featured in ECIR 2022 , the 44th European Conference on Information Retrieval! Check out the new examples on Google Colab! Added a changelog to document few breaking changes introduced in v.0.1.11 .","title":"\ud83d\udd25 News"},{"location":"#introduction","text":"ranx is a library of fast ranking evaluation metrics implemented in Python , leveraging Numba for high-speed vector operations and automatic parallelization. It offers a user-friendly interface to evaluate and compare Information Retrieval and Recommender Systems . Moreover, ranx allows you to perform statistical tests and export LaTeX tables for your scientific publications. For a quick overview, follow the Usage section. For a in-depth overview, follow the Examples section.","title":"\u26a1\ufe0f Introduction"},{"location":"#available-metrics","text":"Hits Hit Rate Precision Recall F1 r-Precision Mean Reciprocal Rank (MRR) Mean Average Precision (MAP) Normalized Discounted Cumulative Gain (NDCG) The metrics have been tested against TREC Eval for correctness.","title":"\u2728 Available Metrics"},{"location":"#installation","text":"pip install ranx","title":"\ud83d\udd0c Installation"},{"location":"#usage","text":"","title":"\ud83d\udca1 Usage"},{"location":"#create-qrels-and-run","text":"from ranx import Qrels , Run , evaluate qrels_dict = { \"q_1\" : { \"d_12\" : 5 , \"d_25\" : 3 }, \"q_2\" : { \"d_11\" : 6 , \"d_22\" : 1 } } run_dict = { \"q_1\" : { \"d_12\" : 0.9 , \"d_23\" : 0.8 , \"d_25\" : 0.7 , \"d_36\" : 0.6 , \"d_32\" : 0.5 , \"d_35\" : 0.4 }, \"q_2\" : { \"d_12\" : 0.9 , \"d_11\" : 0.8 , \"d_25\" : 0.7 , \"d_36\" : 0.6 , \"d_22\" : 0.5 , \"d_35\" : 0.4 } } qrels = Qrels ( qrels_dict ) run = Run ( run_dict )","title":"Create Qrels and Run"},{"location":"#evaluate","text":"# Compute score for a single metric evaluate ( qrels , run , \"ndcg@5\" ) >>> 0.7861 # Compute scores for multiple metrics at once evaluate ( qrels , run , [ \"map@5\" , \"mrr\" ]) >>> { \"map@5\" : 0.6416 , \"mrr\" : 0.75 }","title":"Evaluate"},{"location":"#compare","text":"# Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: >>> # Model MAP@100 MRR@100 NDCG@10 --- ------- -------- -------- --------- a model_1 0.320\u1d47 0.320\u1d47 0.368\u1d47\u1d9c b model_2 0.233 0.234 0.239 c model_3 0.308\u1d47 0.309\u1d47 0.330\u1d47 d model_4 0.366\u1d43\u1d47\u1d9c 0.367\u1d43\u1d47\u1d9c 0.408\u1d43\u1d47\u1d9c e model_5 0.405\u1d43\u1d47\u1d9c\u1d48 0.406\u1d43\u1d47\u1d9c\u1d48 0.451\u1d43\u1d47\u1d9c\u1d48","title":"Compare"},{"location":"#examples","text":"Name Link Overview Qrels and Run Evaluation Comparison and Report","title":"\ud83d\udcd6 Examples"},{"location":"#documentation","text":"Browse the documentation for more details and examples.","title":"\ud83d\udcda Documentation"},{"location":"#citation","text":"If you use ranx to evaluate results for your scientific publication, please consider citing it: @misc{ranx2021, title = {ranx: A Blazing-Fast Python Library for Ranking Evaluation and Comparison}, author = {Bassani, Elias}, year = {2021}, publisher = {GitHub}, howpublished = {\\url{https://github.com/AmenRa/ranx}}, }","title":"\ud83c\udf93 Citation"},{"location":"#feature-requests","text":"Would you like to see other features implemented? Please, open a feature request .","title":"\ud83c\udf81 Feature Requests"},{"location":"#want-to-contribute","text":"Would you like to contribute? Please, drop me an e-mail .","title":"\ud83e\udd18 Want to contribute?"},{"location":"#license","text":"ranx is an open-sourced software licensed under the MIT license .","title":"\ud83d\udcc4 License"},{"location":"evaluate_and_compare/","text":"Evaluate and Compare compare ( qrels , runs , metrics , n_permutations = 1000 , max_p = 0.01 , random_seed = 42 , threads = 0 , rounding_digits = 3 , show_percentages = False ) Evaluate multiple runs and compute statistical tests. Usage example: # Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: # Model MAP@100 MRR@100 NDCG@10 --- ------- ---------- ---------- ---------- a model_1 0.3202\u1d47 0.3207\u1d47 0.3684\u1d47\u1d9c b model_2 0.2332 0.2339 0.239 c model_3 0.3082\u1d47 0.3089\u1d47 0.3295\u1d47 d model_4 0.3664\u1d43\u1d47\u1d9c 0.3668\u1d43\u1d47\u1d9c 0.4078\u1d43\u1d47\u1d9c e model_5 0.4053\u1d43\u1d47\u1d9c\u1d48 0.4061\u1d43\u1d47\u1d9c\u1d48 0.4512\u1d43\u1d47\u1d9c\u1d48 Parameters: Name Type Description Default qrels Qrels Qrels. required runs List[Run] List of runs. required metrics Union[List[str], str] Metric or list of metrics. required n_permutations int Number of permutation to perform during statistical testing (Fisher's Randomization Test is used by default). Defaults to 1000. 1000 max_p float Maximum p-value to consider an increment as statistically significant. Defaults to 0.01. 0.01 random_seed int Random seed to use for generating the permutations. Defaults to 42. 42 threads int Number of threads to use, zero means all the available threads. Defaults to 0. 0 Returns: Type Description Report See report. compute_statistical_significance ( control_metric_scores , treatment_metric_scores , n_permutations = 1000 , max_p = 0.01 , random_seed = 42 ) Used internally. evaluate ( qrels , run , metrics , return_mean = True , threads = 0 , save_results_in_run = True ) Compute the performance scores for the provided qrels and run for all the specified metrics. Usage examples: # Compute score for a single metric evaluate ( qrels , run , \"ndcg@5\" ) >>> 0.7861 # Compute scores for multiple metrics at once evaluate ( qrels , run , [ \"map@5\" , \"mrr\" ]) >>> { \"map@5\" : 0.6416 , \"mrr\" : 0.75 } # Computed metric scores are saved in the Run object run . mean_scores >>> { \"ndcg@5\" : 0.7861 , \"map@5\" : 0.6416 , \"mrr\" : 0.75 } # Access scores for each query dict ( run . scores ) >>> { \"ndcg@5\" : { \"q_1\" : 0.9430 , \"q_2\" : 0.6292 }, \"map@5\" : { \"q_1\" : 0.8333 , \"q_2\" : 0.4500 }, \"mrr\" : { \"q_1\" : 1.0000 , \"q_2\" : 0.5000 }} Parameters: Name Type Description Default qrels Union[ Qrels, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ] Qrels. required run Union[ Run, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ] Run. required metrics Union[List[str], str] Metrics or list of metric to compute. required return_mean bool Wether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True. True threads int Number of threads to use, zero means all the available threads. Defaults to 0. 0 save_results_in_run bool Save metric scores for each query in the input run . Defaults to True. True Returns: Type Description Union[Dict[str, float], float] Results. fuse ( runs , kind = 'wsum' , params = None , norm = 'max' , name = 'fused_run' ) Disclaimer: THIS IS AN EXPERIMENTAL FEATURE! Fuses a list of runs using the specified function and parameters. Only score weighted sum is currently supported. Params must be {weights: weight_list} Parameters: Name Type Description Default runs List[Run] List of runs to fuse. required kind str Fusion function. Only weighted sum is currently supported. Defaults to \"wsum\". 'wsum' params dict Parameters for the fusion function. Defaults to None. None norm str Normalization to apply before fusion. Defaults to \"max\". 'max' name str Name of the fused run. Defaults to \"fused_run\". 'fused_run' Returns: Type Description Run fused run. best_params: optimize_fusion ( qrels , runs , kind = 'wsum' , norm = 'max' , name = 'fused_run' , search_kind = 'greed' , optimize_metric = None , optimize_kwargs = None ) Disclaimer: THIS IS AN EXPERIMENTAL FEATURE!","title":"Evaluate and Compare"},{"location":"evaluate_and_compare/#evaluate-and-compare","text":"","title":"Evaluate and Compare"},{"location":"evaluate_and_compare/#ranx.meta_functions.compare","text":"Evaluate multiple runs and compute statistical tests. Usage example: # Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: # Model MAP@100 MRR@100 NDCG@10 --- ------- ---------- ---------- ---------- a model_1 0.3202\u1d47 0.3207\u1d47 0.3684\u1d47\u1d9c b model_2 0.2332 0.2339 0.239 c model_3 0.3082\u1d47 0.3089\u1d47 0.3295\u1d47 d model_4 0.3664\u1d43\u1d47\u1d9c 0.3668\u1d43\u1d47\u1d9c 0.4078\u1d43\u1d47\u1d9c e model_5 0.4053\u1d43\u1d47\u1d9c\u1d48 0.4061\u1d43\u1d47\u1d9c\u1d48 0.4512\u1d43\u1d47\u1d9c\u1d48 Parameters: Name Type Description Default qrels Qrels Qrels. required runs List[Run] List of runs. required metrics Union[List[str], str] Metric or list of metrics. required n_permutations int Number of permutation to perform during statistical testing (Fisher's Randomization Test is used by default). Defaults to 1000. 1000 max_p float Maximum p-value to consider an increment as statistically significant. Defaults to 0.01. 0.01 random_seed int Random seed to use for generating the permutations. Defaults to 42. 42 threads int Number of threads to use, zero means all the available threads. Defaults to 0. 0 Returns: Type Description Report See report.","title":"compare()"},{"location":"evaluate_and_compare/#ranx.meta_functions.compute_statistical_significance","text":"Used internally.","title":"compute_statistical_significance()"},{"location":"evaluate_and_compare/#ranx.meta_functions.evaluate","text":"Compute the performance scores for the provided qrels and run for all the specified metrics. Usage examples: # Compute score for a single metric evaluate ( qrels , run , \"ndcg@5\" ) >>> 0.7861 # Compute scores for multiple metrics at once evaluate ( qrels , run , [ \"map@5\" , \"mrr\" ]) >>> { \"map@5\" : 0.6416 , \"mrr\" : 0.75 } # Computed metric scores are saved in the Run object run . mean_scores >>> { \"ndcg@5\" : 0.7861 , \"map@5\" : 0.6416 , \"mrr\" : 0.75 } # Access scores for each query dict ( run . scores ) >>> { \"ndcg@5\" : { \"q_1\" : 0.9430 , \"q_2\" : 0.6292 }, \"map@5\" : { \"q_1\" : 0.8333 , \"q_2\" : 0.4500 }, \"mrr\" : { \"q_1\" : 1.0000 , \"q_2\" : 0.5000 }} Parameters: Name Type Description Default qrels Union[ Qrels, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ] Qrels. required run Union[ Run, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ] Run. required metrics Union[List[str], str] Metrics or list of metric to compute. required return_mean bool Wether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True. True threads int Number of threads to use, zero means all the available threads. Defaults to 0. 0 save_results_in_run bool Save metric scores for each query in the input run . Defaults to True. True Returns: Type Description Union[Dict[str, float], float] Results.","title":"evaluate()"},{"location":"evaluate_and_compare/#ranx.meta_functions.fuse","text":"Disclaimer: THIS IS AN EXPERIMENTAL FEATURE! Fuses a list of runs using the specified function and parameters. Only score weighted sum is currently supported. Params must be {weights: weight_list} Parameters: Name Type Description Default runs List[Run] List of runs to fuse. required kind str Fusion function. Only weighted sum is currently supported. Defaults to \"wsum\". 'wsum' params dict Parameters for the fusion function. Defaults to None. None norm str Normalization to apply before fusion. Defaults to \"max\". 'max' name str Name of the fused run. Defaults to \"fused_run\". 'fused_run' Returns: Type Description Run fused run. best_params:","title":"fuse()"},{"location":"evaluate_and_compare/#ranx.meta_functions.optimize_fusion","text":"Disclaimer: THIS IS AN EXPERIMENTAL FEATURE!","title":"optimize_fusion()"},{"location":"metrics/","text":"Metrics Aliases to use with ranx.evaluate and ranx.compare . hits hit_rate precision recall f1 r-precision mrr map ndcg ndcg_burges Internal implementation of the provided metrics. Please refers to evaluate for usage. average_precision ( qrels , run , k = 0 ) Compute Average Precision. Average Precision is the average of the Precision scores computed after each relevant document is retrieved. If k > 0, only the top-k retrieved documents are considered. \\[ \\operatorname{Average Precision} = \\frac{\\sum_r \\operatorname{Precision}@r}{R} \\] where, \\(r\\) is the position of a relevant document; \\(R\\) is the total number of relevant documents. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray Average Precision (at k) scores. f1 ( qrels , run , k = 0 ) Compute F1 (at k). F1 is the harmonic mean of Precision and Recall . If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{F1} = 2 \\times \\frac{\\operatorname{Precision} \\times \\operatorname{Recall}}{\\operatorname{Precision} + \\operatorname{Recall}} \\] If k > 0, \\[ \\operatorname{F1@k} = 2 \\times \\frac{\\operatorname{Precision@k} \\times \\operatorname{Recall@k}}{\\operatorname{Precision@k} + \\operatorname{Recall@k}} \\] Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray F1 (at k) scores. hit_rate ( qrels , run , k = 0 ) Compute Hit Rate (at k). Hit Rate is the fraction of queries for which at least one relevant document is retrieved. If k > 0, only the top-k retrieved documents are considered. Note: it is equivalent to success from trec_eval . Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray Hit Rate (at k) scores. hits ( qrels , run , k = 0 ) Compute Hits (at k). Hits is the number of relevant documents retrieved. If k > 0, only the top-k retrieved documents are considered. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray Hits (at k) scores. ndcg ( qrels , run , k = 0 ) Compute Normalized Discounted Cumulative Gain (NDCG) as proposed by J\u00e4rvelin et al. . If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] where, \\(\\operatorname{DCG}\\) is Discounted Cumulative Gain; \\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possibile DCG). If k > 0, \\[ \\operatorname{nDCG}_k = \\frac{\\operatorname{DCG}_k}{\\operatorname{IDCG}_k} \\] where, \\(\\operatorname{DCG}_k\\) is Discounted Cumulative Gain at k; \\(\\operatorname{IDCG}_k\\) is Ideal Discounted Cumulative Gain at k (max possibile DCG at k). @article { DBLP:journals/tois/JarvelinK02 , author = {Kalervo J{\\\"{a}}rvelin and Jaana Kek{\\\"{a}}l{\\\"{a}}inen} , title = {Cumulated gain-based evaluation of {IR} techniques} , journal = {{ACM} Trans. Inf. Syst.} , volume = {20} , number = {4} , pages = {422--446} , year = {2002} } Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray Normalized Discounted Cumulative Gain (at k) scores. ndcg_burges ( qrels , run , k = 0 ) Compute Normalized Discounted Cumulative Gain (NDCG) at k as proposed by Burges et al. . If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] where, \\(\\operatorname{DCG}\\) is Discounted Cumulative Gain; \\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possibile DCG). If k > 0, \\[ \\operatorname{nDCG}_k = \\frac{\\operatorname{DCG}_k}{\\operatorname{IDCG}_k} \\] where, \\(\\operatorname{DCG}_k\\) is Discounted Cumulative Gain at k; \\(\\operatorname{IDCG}_k\\) is Ideal Discounted Cumulative Gain at k (max possibile DCG at k). @inproceedings { DBLP:conf/icml/BurgesSRLDHH05 , author = {Christopher J. C. Burges and Tal Shaked and Erin Renshaw and Ari Lazier and Matt Deeds and Nicole Hamilton and Gregory N. Hullender} , title = {Learning to rank using gradient descent} , booktitle = {{ICML}} , series = {{ACM} International Conference Proceeding Series} , volume = {119} , pages = {89--96} , publisher = {{ACM}} , year = {2005} } Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray Normalized Discounted Cumulative Gain (at k) scores. precision ( qrels , run , k = 0 ) Compute Precision (at k). Precision is the proportion of the retrieved documents that are relevant. If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{Precision}=\\frac{r}{n} \\] where, \\(r\\) is the number of retrieved relevant documents; \\(n\\) is the number of retrieved documents. If k > 0, \\[ \\operatorname{Precision@k}=\\frac{r_k}{k} \\] where, \\(r_k\\) is the number of retrieved relevant documents at k. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray Precision (at k) scores. r_precision ( qrels , run , k = 0 ) Compute R-Precision. For a given query \\(Q\\) , R-Precision is the precision at \\(R\\) , where \\(R\\) is the number of relevant documents for \\(Q\\) . In other words, if there are \\(r\\) relevant documents among the top- \\(R\\) retrieved documents, then R-precision is: \\[ \\operatorname{R-Precision} = \\frac{r}{R} \\] Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray R-Precision scores. recall ( qrels , run , k = 0 ) Compute Recall (at k). Recall is the ratio between the retrieved documents that are relevant and the total number of relevant documents. If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{Recall}=\\frac{r}{R} \\] where, \\(r\\) is the number of retrieved relevant documents; \\(R\\) is the total number of relevant documents. If k > 0, \\[ \\operatorname{Recall@k}=\\frac{r_k}{R} \\] where, \\(r_k\\) is the number of retrieved relevant documents at k; \\(R\\) is the total number of relevant documents. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray Recall (at k) scores. reciprocal_rank ( qrels , run , k = 0 ) Compute Reciprocal Rank (at k). The Reciprocal Rank is the multiplicative inverse of the rank of the first retrieved relevant document: 1 for first place, 1/2 for second place, 1/3 for third place, and so on. If k > 0, only the top-k retrieved documents are considered. \\[ Reciprocal Rank = \\frac{1}{rank} \\] where, \\(rank\\) is the position of the first retrieved relevant document. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray Reciprocal Rank (at k) scores.","title":"Metrics"},{"location":"metrics/#metrics","text":"Aliases to use with ranx.evaluate and ranx.compare . hits hit_rate precision recall f1 r-precision mrr map ndcg ndcg_burges Internal implementation of the provided metrics. Please refers to evaluate for usage.","title":"Metrics"},{"location":"metrics/#ranx.metrics.average_precision","text":"Compute Average Precision. Average Precision is the average of the Precision scores computed after each relevant document is retrieved. If k > 0, only the top-k retrieved documents are considered. \\[ \\operatorname{Average Precision} = \\frac{\\sum_r \\operatorname{Precision}@r}{R} \\] where, \\(r\\) is the position of a relevant document; \\(R\\) is the total number of relevant documents. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray Average Precision (at k) scores.","title":"average_precision()"},{"location":"metrics/#ranx.metrics.f1","text":"Compute F1 (at k). F1 is the harmonic mean of Precision and Recall . If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{F1} = 2 \\times \\frac{\\operatorname{Precision} \\times \\operatorname{Recall}}{\\operatorname{Precision} + \\operatorname{Recall}} \\] If k > 0, \\[ \\operatorname{F1@k} = 2 \\times \\frac{\\operatorname{Precision@k} \\times \\operatorname{Recall@k}}{\\operatorname{Precision@k} + \\operatorname{Recall@k}} \\] Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray F1 (at k) scores.","title":"f1()"},{"location":"metrics/#ranx.metrics.hit_rate","text":"Compute Hit Rate (at k). Hit Rate is the fraction of queries for which at least one relevant document is retrieved. If k > 0, only the top-k retrieved documents are considered. Note: it is equivalent to success from trec_eval . Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray Hit Rate (at k) scores.","title":"hit_rate()"},{"location":"metrics/#ranx.metrics.hits","text":"Compute Hits (at k). Hits is the number of relevant documents retrieved. If k > 0, only the top-k retrieved documents are considered. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray Hits (at k) scores.","title":"hits()"},{"location":"metrics/#ranx.metrics.ndcg","text":"Compute Normalized Discounted Cumulative Gain (NDCG) as proposed by J\u00e4rvelin et al. . If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] where, \\(\\operatorname{DCG}\\) is Discounted Cumulative Gain; \\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possibile DCG). If k > 0, \\[ \\operatorname{nDCG}_k = \\frac{\\operatorname{DCG}_k}{\\operatorname{IDCG}_k} \\] where, \\(\\operatorname{DCG}_k\\) is Discounted Cumulative Gain at k; \\(\\operatorname{IDCG}_k\\) is Ideal Discounted Cumulative Gain at k (max possibile DCG at k). @article { DBLP:journals/tois/JarvelinK02 , author = {Kalervo J{\\\"{a}}rvelin and Jaana Kek{\\\"{a}}l{\\\"{a}}inen} , title = {Cumulated gain-based evaluation of {IR} techniques} , journal = {{ACM} Trans. Inf. Syst.} , volume = {20} , number = {4} , pages = {422--446} , year = {2002} } Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray Normalized Discounted Cumulative Gain (at k) scores.","title":"ndcg()"},{"location":"metrics/#ranx.metrics.ndcg_burges","text":"Compute Normalized Discounted Cumulative Gain (NDCG) at k as proposed by Burges et al. . If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] where, \\(\\operatorname{DCG}\\) is Discounted Cumulative Gain; \\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possibile DCG). If k > 0, \\[ \\operatorname{nDCG}_k = \\frac{\\operatorname{DCG}_k}{\\operatorname{IDCG}_k} \\] where, \\(\\operatorname{DCG}_k\\) is Discounted Cumulative Gain at k; \\(\\operatorname{IDCG}_k\\) is Ideal Discounted Cumulative Gain at k (max possibile DCG at k). @inproceedings { DBLP:conf/icml/BurgesSRLDHH05 , author = {Christopher J. C. Burges and Tal Shaked and Erin Renshaw and Ari Lazier and Matt Deeds and Nicole Hamilton and Gregory N. Hullender} , title = {Learning to rank using gradient descent} , booktitle = {{ICML}} , series = {{ACM} International Conference Proceeding Series} , volume = {119} , pages = {89--96} , publisher = {{ACM}} , year = {2005} } Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray Normalized Discounted Cumulative Gain (at k) scores.","title":"ndcg_burges()"},{"location":"metrics/#ranx.metrics.precision","text":"Compute Precision (at k). Precision is the proportion of the retrieved documents that are relevant. If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{Precision}=\\frac{r}{n} \\] where, \\(r\\) is the number of retrieved relevant documents; \\(n\\) is the number of retrieved documents. If k > 0, \\[ \\operatorname{Precision@k}=\\frac{r_k}{k} \\] where, \\(r_k\\) is the number of retrieved relevant documents at k. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray Precision (at k) scores.","title":"precision()"},{"location":"metrics/#ranx.metrics.r_precision","text":"Compute R-Precision. For a given query \\(Q\\) , R-Precision is the precision at \\(R\\) , where \\(R\\) is the number of relevant documents for \\(Q\\) . In other words, if there are \\(r\\) relevant documents among the top- \\(R\\) retrieved documents, then R-precision is: \\[ \\operatorname{R-Precision} = \\frac{r}{R} \\] Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray R-Precision scores.","title":"r_precision()"},{"location":"metrics/#ranx.metrics.recall","text":"Compute Recall (at k). Recall is the ratio between the retrieved documents that are relevant and the total number of relevant documents. If k > 0, only the top-k retrieved documents are considered. If k = 0, \\[ \\operatorname{Recall}=\\frac{r}{R} \\] where, \\(r\\) is the number of retrieved relevant documents; \\(R\\) is the total number of relevant documents. If k > 0, \\[ \\operatorname{Recall@k}=\\frac{r_k}{R} \\] where, \\(r_k\\) is the number of retrieved relevant documents at k; \\(R\\) is the total number of relevant documents. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int Number of retrieved documents to consider. k=0 means all retrieved documents will be considered. Defaults to 0. 0 Returns: Type Description ndarray Recall (at k) scores.","title":"recall()"},{"location":"metrics/#ranx.metrics.reciprocal_rank","text":"Compute Reciprocal Rank (at k). The Reciprocal Rank is the multiplicative inverse of the rank of the first retrieved relevant document: 1 for first place, 1/2 for second place, 1/3 for third place, and so on. If k > 0, only the top-k retrieved documents are considered. \\[ Reciprocal Rank = \\frac{1}{rank} \\] where, \\(rank\\) is the position of the first retrieved relevant document. Parameters: Name Type Description Default qrels Union[np.ndarray, numba.typed.List] IDs and relevance scores of relevant documents. required run Union[np.ndarray, numba.typed.List] IDs and relevance scores of retrieved documents. required k int This argument is ignored. It was added to standardize metrics' input. Defaults to 0. 0 Returns: Type Description ndarray Reciprocal Rank (at k) scores.","title":"reciprocal_rank()"},{"location":"qrels_and_run%20copy/","text":"Report Report stores the results of a comparison. Report A Report instance is automatically generated as the results of a comparison. A Report provide a convenient way of inspecting a comparison results and exporting those il LaTeX for your scientific publications. # Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: # Model MAP@100 MRR@100 NDCG@10 --- ------- ---------- ---------- ---------- a model_1 0.3202\u1d47 0.3207\u1d47 0.3684\u1d47\u1d9c b model_2 0.2332 0.2339 0.239 c model_3 0.3082\u1d47 0.3089\u1d47 0.3295\u1d47 d model_4 0.3664\u1d43\u1d47\u1d9c 0.3668\u1d43\u1d47\u1d9c 0.4078\u1d43\u1d47\u1d9c e model_5 0.4053\u1d43\u1d47\u1d9c\u1d48 0.4061\u1d43\u1d47\u1d9c\u1d48 0.4512\u1d43\u1d47\u1d9c\u1d48 print ( report . to_latex ()) # To get the LaTeX code print_results ( self ) Print report data. save ( self , path ) Save the Report data as JSON file. See Report.to_dict for more details. Parameters: Name Type Description Default path str Saving path required to_dict ( self ) Returns the Report data as a Python dictionary. { # metrics and model_names allows to read the report without # inspecting the json to discover the used metrics and # the compared models \"metrics\" : [ \"metric_1\" , \"metric_2\" , ... ], \"model_names\" : [ \"model_1\" , \"model_2\" , ... ], # \"model_1\" : { \"scores\" : { \"metric_1\" : ... , \"metric_2\" : ... , ... }, \"comparisons\" : { \"model_2\" : { \"metric_1\" : ... , # p-value \"metric_2\" : ... , # p-value ... }, ... }, \"win_tie_loss\" : { \"model_2\" : { \"W\" : ... , \"T\" : ... , \"L\" : ... , }, ... }, }, ... } Returns: Type Description Dict Report data as a Python dictionary to_latex ( self ) Returns Report as LaTeX table. Returns: Type Description str LaTeX table","title":"Report"},{"location":"qrels_and_run%20copy/#report","text":"Report stores the results of a comparison.","title":"Report"},{"location":"qrels_and_run%20copy/#ranx.report.Report","text":"A Report instance is automatically generated as the results of a comparison. A Report provide a convenient way of inspecting a comparison results and exporting those il LaTeX for your scientific publications. # Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: # Model MAP@100 MRR@100 NDCG@10 --- ------- ---------- ---------- ---------- a model_1 0.3202\u1d47 0.3207\u1d47 0.3684\u1d47\u1d9c b model_2 0.2332 0.2339 0.239 c model_3 0.3082\u1d47 0.3089\u1d47 0.3295\u1d47 d model_4 0.3664\u1d43\u1d47\u1d9c 0.3668\u1d43\u1d47\u1d9c 0.4078\u1d43\u1d47\u1d9c e model_5 0.4053\u1d43\u1d47\u1d9c\u1d48 0.4061\u1d43\u1d47\u1d9c\u1d48 0.4512\u1d43\u1d47\u1d9c\u1d48 print ( report . to_latex ()) # To get the LaTeX code","title":"Report"},{"location":"qrels_and_run%20copy/#ranx.report.Report.print_results","text":"Print report data.","title":"print_results()"},{"location":"qrels_and_run%20copy/#ranx.report.Report.save","text":"Save the Report data as JSON file. See Report.to_dict for more details. Parameters: Name Type Description Default path str Saving path required","title":"save()"},{"location":"qrels_and_run%20copy/#ranx.report.Report.to_dict","text":"Returns the Report data as a Python dictionary. { # metrics and model_names allows to read the report without # inspecting the json to discover the used metrics and # the compared models \"metrics\" : [ \"metric_1\" , \"metric_2\" , ... ], \"model_names\" : [ \"model_1\" , \"model_2\" , ... ], # \"model_1\" : { \"scores\" : { \"metric_1\" : ... , \"metric_2\" : ... , ... }, \"comparisons\" : { \"model_2\" : { \"metric_1\" : ... , # p-value \"metric_2\" : ... , # p-value ... }, ... }, \"win_tie_loss\" : { \"model_2\" : { \"W\" : ... , \"T\" : ... , \"L\" : ... , }, ... }, }, ... } Returns: Type Description Dict Report data as a Python dictionary","title":"to_dict()"},{"location":"qrels_and_run%20copy/#ranx.report.Report.to_latex","text":"Returns Report as LaTeX table. Returns: Type Description str LaTeX table","title":"to_latex()"},{"location":"qrels_and_run/","text":"Qrels and Run The first step in the offline evaluation of the effectiveness of an IR system is the definition of a list of query relevance judgments and the ranked lists of documents retrieved for those queries by the system. To ease the managing of these data, ranx implements two dedicated Python classes: 1) Qrels for the query relevance judgments and 2) Run for the computed ranked lists. Qrels Qrels , or query relevance judgments , stores the ground truth for conducting evaluations. The preferred way for creating a Qrels istance is converting Python dictionary as follows: qrels_dict = { \"q_1\" : { \"d_1\" : 1 , \"d_2\" : 2 , }, \"q_2\" : { \"d_3\" : 2 , \"d_2\" : 1 , \"d_5\" : 3 , }, } qrels = Qrels ( qrels_dict , name = \"MSMARCO\" ) qrels = Qrels () # Creates an empty Qrels with no name add ( self , q_id , doc_ids , scores ) Add a query and its relevant documents with the associated relevance score judgment. Parameters: Name Type Description Default q_id str Query ID required doc_ids List[str] List of Document IDs required scores List[int] List of relevance score judgments required add_multi ( self , q_ids , doc_ids , scores ) Add multiple queries at once. Parameters: Name Type Description Default q_ids List[str] List of Query IDs required doc_ids List[List[str]] List of list of Document IDs required scores List[List[int]] List of list of relevance score judgments required add_score ( self , q_id , doc_id , score ) Add a (doc_id, score) pair to a query (or, change its value if it already exists). Parameters: Name Type Description Default q_id str Query ID required doc_id str Document ID required score int Relevance score judgment required from_df ( df , q_id_col = 'q_id' , doc_id_col = 'doc_id' , score_col = 'score' ) staticmethod Convert a Pandas DataFrame to ranx.Qrels. Parameters: Name Type Description Default df pd.DataFrame Qrels as Pandas DataFrame required q_id_col str Query IDs column. Defaults to \"q_id\". 'q_id' doc_id_col str Document IDs column. Defaults to \"doc_id\". 'doc_id' score_col str Relevance score judgments column. Defaults to \"score\". 'score' Returns: Type Description Qrels ranx.Qrels from_dict ( d ) staticmethod Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Qrels. Parameters: Name Type Description Default d Dict[str, Dict[str, int]] Qrels as Python dictionary required Returns: Type Description Qrels ranx.Qrels from_file ( path , kind = 'json' ) staticmethod Parse a qrels file into ranx.Qrels. Supported formats are JSON and TREC qrels format. Parameters: Name Type Description Default path str File path. required kind str Kind of file to load, must be either \"json\" or \"trec\". Defaults to \"json\". 'json' Returns: Type Description Qrels ranx.Qrels get_doc_ids_and_scores ( self ) Returns doc ids and relevance judgments. get_query_ids ( self ) Returns query ids. keys ( self ) Returns query ids. Used internally. save ( self , path = 'qrels.json' , kind = 'json' ) Write qrels to path as JSON file or TREC qrels format. Parameters: Name Type Description Default path str Saving path. Defaults to \"qrels.json\". 'qrels.json' kind str Kind of file to save, must be either \"json\" or \"trec\". Defaults to \"json\". 'json' sort ( self ) Sort. Used internally. to_dict ( self ) Convert Qrels to Python dictionary. Returns: Type Description Dict[str, Dict[str, int]] Qrels as Python dictionary to_typed_list ( self ) Convert Qrels to Numba Typed List. Used internally. Run Run stores the relevance scores estimated by the model under evaluation. The preferred way for creating a Run istance is converting a Python dictionary as follows: run_dict = { \"q_1\" : { \"d_1\" : 1.5 , \"d_2\" : 2.6 , }, \"q_2\" : { \"d_3\" : 2.8 , \"d_2\" : 1.2 , \"d_5\" : 3.1 , }, } run = Run ( run_dict , name = \"bm25\" ) run = Run () # Creates an empty Run with no name add ( self , q_id , doc_ids , scores ) Add a query and its relevant documents with the associated relevance score. Parameters: Name Type Description Default q_id str Query ID required doc_ids List[str] List of Document IDs required scores List[int] List of relevance scores required add_multi ( self , q_ids , doc_ids , scores ) Add multiple queries at once. Parameters: Name Type Description Default q_ids List[str] List of Query IDs required doc_ids List[List[str]] List of list of Document IDs required scores List[List[int]] List of list of relevance scores required add_score ( self , q_id , doc_id , score ) Add a (doc_id, score) pair to a query (or, change its value if it already exists). Parameters: Name Type Description Default q_id str Query ID required doc_id str Document ID required score int Relevance score required from_df ( df , q_id_col = 'q_id' , doc_id_col = 'doc_id' , score_col = 'score' ) staticmethod Convert a Pandas DataFrame to ranx.Run. Parameters: Name Type Description Default df pd.DataFrame Run as Pandas DataFrame required q_id_col str Query IDs column. Defaults to \"q_id\". 'q_id' doc_id_col str Document IDs column. Defaults to \"doc_id\". 'doc_id' score_col str Relevance scores column. Defaults to \"score\". 'score' Returns: Type Description Run ranx.Run from_dict ( d ) staticmethod Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Run. Parameters: Name Type Description Default d Dict[str, Dict[str, int]] Run as Python dictionary required Returns: Type Description Run ranx.Run from_file ( path , kind = 'json' ) staticmethod Parse a run file into ranx.Run. Supported formats are JSON and TREC run format. Parameters: Name Type Description Default path str File path. required kind str Kind of file to load, must be either \"json\" or \"trec\". Defaults to \"json\". 'json' Returns: Type Description Run ranx.Run get_doc_ids_and_scores ( self ) Returns doc ids and relevance scores. get_query_ids ( self ) Returns query ids. keys ( self ) Returns query ids. Used internally. save ( self , path = 'run.json' , kind = 'json' ) Write run to path as JSON file or TREC run format. Parameters: Name Type Description Default path str Saving path. Defaults to \"run.json\". 'run.json' kind str Kind of file to save, must be either \"json\" or \"trec\". Defaults to \"json\". 'json' sort ( self ) Sort. Used internally. to_dict ( self ) Convert Run to Python dictionary. Returns: Type Description Dict[str, Dict[str, int]] Run as Python dictionary to_typed_list ( self ) Convert Run to Numba Typed List. Used internally.","title":"Qrels and Run"},{"location":"qrels_and_run/#qrels-and-run","text":"The first step in the offline evaluation of the effectiveness of an IR system is the definition of a list of query relevance judgments and the ranked lists of documents retrieved for those queries by the system. To ease the managing of these data, ranx implements two dedicated Python classes: 1) Qrels for the query relevance judgments and 2) Run for the computed ranked lists.","title":"Qrels and Run"},{"location":"qrels_and_run/#ranx.qrels.Qrels","text":"Qrels , or query relevance judgments , stores the ground truth for conducting evaluations. The preferred way for creating a Qrels istance is converting Python dictionary as follows: qrels_dict = { \"q_1\" : { \"d_1\" : 1 , \"d_2\" : 2 , }, \"q_2\" : { \"d_3\" : 2 , \"d_2\" : 1 , \"d_5\" : 3 , }, } qrels = Qrels ( qrels_dict , name = \"MSMARCO\" ) qrels = Qrels () # Creates an empty Qrels with no name","title":"Qrels"},{"location":"qrels_and_run/#ranx.qrels.Qrels.add","text":"Add a query and its relevant documents with the associated relevance score judgment. Parameters: Name Type Description Default q_id str Query ID required doc_ids List[str] List of Document IDs required scores List[int] List of relevance score judgments required","title":"add()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.add_multi","text":"Add multiple queries at once. Parameters: Name Type Description Default q_ids List[str] List of Query IDs required doc_ids List[List[str]] List of list of Document IDs required scores List[List[int]] List of list of relevance score judgments required","title":"add_multi()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.add_score","text":"Add a (doc_id, score) pair to a query (or, change its value if it already exists). Parameters: Name Type Description Default q_id str Query ID required doc_id str Document ID required score int Relevance score judgment required","title":"add_score()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.from_df","text":"Convert a Pandas DataFrame to ranx.Qrels. Parameters: Name Type Description Default df pd.DataFrame Qrels as Pandas DataFrame required q_id_col str Query IDs column. Defaults to \"q_id\". 'q_id' doc_id_col str Document IDs column. Defaults to \"doc_id\". 'doc_id' score_col str Relevance score judgments column. Defaults to \"score\". 'score' Returns: Type Description Qrels ranx.Qrels","title":"from_df()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.from_dict","text":"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Qrels. Parameters: Name Type Description Default d Dict[str, Dict[str, int]] Qrels as Python dictionary required Returns: Type Description Qrels ranx.Qrels","title":"from_dict()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.from_file","text":"Parse a qrels file into ranx.Qrels. Supported formats are JSON and TREC qrels format. Parameters: Name Type Description Default path str File path. required kind str Kind of file to load, must be either \"json\" or \"trec\". Defaults to \"json\". 'json' Returns: Type Description Qrels ranx.Qrels","title":"from_file()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.get_doc_ids_and_scores","text":"Returns doc ids and relevance judgments.","title":"get_doc_ids_and_scores()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.get_query_ids","text":"Returns query ids.","title":"get_query_ids()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.keys","text":"Returns query ids. Used internally.","title":"keys()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.save","text":"Write qrels to path as JSON file or TREC qrels format. Parameters: Name Type Description Default path str Saving path. Defaults to \"qrels.json\". 'qrels.json' kind str Kind of file to save, must be either \"json\" or \"trec\". Defaults to \"json\". 'json'","title":"save()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.sort","text":"Sort. Used internally.","title":"sort()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.to_dict","text":"Convert Qrels to Python dictionary. Returns: Type Description Dict[str, Dict[str, int]] Qrels as Python dictionary","title":"to_dict()"},{"location":"qrels_and_run/#ranx.qrels.Qrels.to_typed_list","text":"Convert Qrels to Numba Typed List. Used internally.","title":"to_typed_list()"},{"location":"qrels_and_run/#ranx.run.Run","text":"Run stores the relevance scores estimated by the model under evaluation. The preferred way for creating a Run istance is converting a Python dictionary as follows: run_dict = { \"q_1\" : { \"d_1\" : 1.5 , \"d_2\" : 2.6 , }, \"q_2\" : { \"d_3\" : 2.8 , \"d_2\" : 1.2 , \"d_5\" : 3.1 , }, } run = Run ( run_dict , name = \"bm25\" ) run = Run () # Creates an empty Run with no name","title":"Run"},{"location":"qrels_and_run/#ranx.run.Run.add","text":"Add a query and its relevant documents with the associated relevance score. Parameters: Name Type Description Default q_id str Query ID required doc_ids List[str] List of Document IDs required scores List[int] List of relevance scores required","title":"add()"},{"location":"qrels_and_run/#ranx.run.Run.add_multi","text":"Add multiple queries at once. Parameters: Name Type Description Default q_ids List[str] List of Query IDs required doc_ids List[List[str]] List of list of Document IDs required scores List[List[int]] List of list of relevance scores required","title":"add_multi()"},{"location":"qrels_and_run/#ranx.run.Run.add_score","text":"Add a (doc_id, score) pair to a query (or, change its value if it already exists). Parameters: Name Type Description Default q_id str Query ID required doc_id str Document ID required score int Relevance score required","title":"add_score()"},{"location":"qrels_and_run/#ranx.run.Run.from_df","text":"Convert a Pandas DataFrame to ranx.Run. Parameters: Name Type Description Default df pd.DataFrame Run as Pandas DataFrame required q_id_col str Query IDs column. Defaults to \"q_id\". 'q_id' doc_id_col str Document IDs column. Defaults to \"doc_id\". 'doc_id' score_col str Relevance scores column. Defaults to \"score\". 'score' Returns: Type Description Run ranx.Run","title":"from_df()"},{"location":"qrels_and_run/#ranx.run.Run.from_dict","text":"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Run. Parameters: Name Type Description Default d Dict[str, Dict[str, int]] Run as Python dictionary required Returns: Type Description Run ranx.Run","title":"from_dict()"},{"location":"qrels_and_run/#ranx.run.Run.from_file","text":"Parse a run file into ranx.Run. Supported formats are JSON and TREC run format. Parameters: Name Type Description Default path str File path. required kind str Kind of file to load, must be either \"json\" or \"trec\". Defaults to \"json\". 'json' Returns: Type Description Run ranx.Run","title":"from_file()"},{"location":"qrels_and_run/#ranx.run.Run.get_doc_ids_and_scores","text":"Returns doc ids and relevance scores.","title":"get_doc_ids_and_scores()"},{"location":"qrels_and_run/#ranx.run.Run.get_query_ids","text":"Returns query ids.","title":"get_query_ids()"},{"location":"qrels_and_run/#ranx.run.Run.keys","text":"Returns query ids. Used internally.","title":"keys()"},{"location":"qrels_and_run/#ranx.run.Run.save","text":"Write run to path as JSON file or TREC run format. Parameters: Name Type Description Default path str Saving path. Defaults to \"run.json\". 'run.json' kind str Kind of file to save, must be either \"json\" or \"trec\". Defaults to \"json\". 'json'","title":"save()"},{"location":"qrels_and_run/#ranx.run.Run.sort","text":"Sort. Used internally.","title":"sort()"},{"location":"qrels_and_run/#ranx.run.Run.to_dict","text":"Convert Run to Python dictionary. Returns: Type Description Dict[str, Dict[str, int]] Run as Python dictionary","title":"to_dict()"},{"location":"qrels_and_run/#ranx.run.Run.to_typed_list","text":"Convert Run to Numba Typed List. Used internally.","title":"to_typed_list()"}]}